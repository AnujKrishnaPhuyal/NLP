{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "import spacy\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint\n",
    "from torchtext.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy models for German and English\n",
    "spacy_ger = spacy.load(\"de_core_news_sm\")\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making tokenizer function \n",
    "\n",
    "text = \"I am a handsome guy\"\n",
    "\n",
    "def tok_english(text):\n",
    "     return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "def tok_german(text):\n",
    "     return [tok.text for tok in spacy_ger.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng text is: ['I', 'am', 'a', 'handsome', 'guy']\n",
      "german text is: ['Ceci', 'est', 'un', 'texte', \"d'\", 'exemple', '.']\n"
     ]
    }
   ],
   "source": [
    "# Testing the tokenizer function\n",
    "print(f\"eng text is: {tok_english(text)}\")\n",
    "ger= tok_german(\"Ceci est un texte d' exemple.\")\n",
    "print(f\"german text is: {ger}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #using field for structuring all the given datas into lower case, star of sentences and end of sentences\n",
    "\n",
    "# english = Field(tokenize=tok_english, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "# german = Field(tokenize=tok_german, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #loading the data from our datasets into our training part\n",
    "\n",
    "# english_train_data, = TabularDataset.splits(\n",
    "#     path=\"./datasets/train\",\n",
    "#     train='eng.en',\n",
    "#     format='tsv',  # You can set format to 'csv' or 'tsv', it doesn't matter\n",
    "#     fields=[('english',english)]\n",
    "# )\n",
    "# german_train_data, = TabularDataset.splits(\n",
    "#     path=\"./datasets/train/\",\n",
    "#     test='ger.de',  # Specify the English file name for the test dataset\n",
    "#     format='csv',  # You can set format to 'csv' or 'tsv', it doesn't matter\n",
    "#     fields=[('german', german)]\n",
    "# )\n",
    "\n",
    "# #for testing part\n",
    "# english_test_data, = TabularDataset.splits(\n",
    "#     path=\"./datasets/test/\",\n",
    "#     test='eng.en',  # Specify the English file name for the test dataset\n",
    "#     format='csv',  # You can set format to 'csv' or 'tsv', it doesn't matter\n",
    "#     fields=[('english',english)])\n",
    "\n",
    "# german_test_data, = TabularDataset.splits(\n",
    "#     path=\"./datasets/test/\",\n",
    "#     test='ger.de',  # Specify the English file name for the test dataset\n",
    "#     format='csv',  # You can set format to 'csv' or 'tsv', it doesn't matter\n",
    "#     fields=[('german', german)]\n",
    "# )\n",
    "\n",
    "# #for validation part\n",
    "# english_validation_data, = TabularDataset.splits(\n",
    "#     path=\"./datasets/valid/\",\n",
    "#     test='eng.en',  # Specify the English file name for the test dataset\n",
    "#     format='csv',  # You can set format to 'csv' or 'tsv', it doesn't matter\n",
    "#     fields=[('english', english)]\n",
    "# )\n",
    "# german_validation_data, = TabularDataset.splits(\n",
    "#     path=\"./datasets/valid/\",\n",
    "#     test='ger.de',  # Specify the English file name for the test dataset\n",
    "#     format='csv',  # You can set format to 'csv' or 'tsv', it doesn't matter\n",
    "#     fields=[('german', german)]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #checking out  datas from our datasets\n",
    "# for i, example in enumerate(german_validation_data):\n",
    "#     print(\"german Text:\", \" \".join(example.german))\n",
    "#     # print(example.german)\n",
    "#     if i==3:\n",
    "#         break\n",
    "\n",
    "# print(\"___________________________________________\") \n",
    " \n",
    "# for i, example in enumerate(english_validation_data):\n",
    "#     print(\"english Text:\", \" \".join(example.english))\n",
    "#     # print(example.english)\n",
    "#     if i==3:\n",
    "#         break    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read English and German files\n",
    "with open('datasets/train/eng.en', 'r', encoding='utf-8') as f:\n",
    "    train_english_sentences = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open('datasets/train/ger.de', 'r', encoding='utf-8') as f:\n",
    "    train_german_sentences = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open('datasets/test/eng.en', 'r', encoding='utf-8') as f:\n",
    "    test_english_sentences = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open('datasets/test/ger.de', 'r', encoding='utf-8') as f:\n",
    "    test_german_sentences = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open('datasets/valid/eng.en', 'r', encoding='utf-8') as f:\n",
    "    valid_english_sentences = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open('datasets/valid/ger.de', 'r', encoding='utf-8') as f:\n",
    "    valid_german_sentences = [line.strip() for line in f.readlines()]        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pairs\n",
    "train_data = list(zip(train_german_sentences,train_english_sentences))\n",
    "valid_data = list(zip(valid_german_sentences,valid_english_sentences))\n",
    "test_data = list(zip(test_german_sentences,test_english_sentences))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zwei männer betrachten etwas im garten',\n",
       "  'two young guys with shaggy hair look at their hands while hanging out in the yard .'),\n",
       " ('die männer arbeiten an der seilbahn .',\n",
       "  'several men in hard hats are operating a giant pulley system .'),\n",
       " ('ein mädchen im rosa kleid klettert in eine stall',\n",
       "  'a child in a pink dress is climbing up a set of stairs in an entry way .'),\n",
       " ('ein mann auf einer leiter putzt ein fenster',\n",
       "  'someone in a blue shirt and hat is standing on stair and leaning against a window .'),\n",
       " ('ein mann am herd füllt den teller eines zweiten mannes .',\n",
       "  'two men , one in a gray shirt , one in a black shirt , standing near a stove .')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using field for structuring all the given datas into lower case, star of sentences and end of sentences\n",
    "\n",
    "english = Field(tokenize=tok_english, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "german = Field(tokenize=tok_german, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields=[('german',german),('english',english)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create train datasets\n",
    "train_examples = [torchtext.data.Example.fromlist([pair[0], pair[1]], fields) for pair in train_data]\n",
    "train_dataset = torchtext.data.Dataset(train_examples, fields)\n",
    "\n",
    "\n",
    "# Create test datasets\n",
    "test_examples = [torchtext.data.Example.fromlist([pair[0], pair[1]], fields) for pair in test_data]\n",
    "test_dataset = torchtext.data.Dataset(test_examples, fields)\n",
    "\n",
    "\n",
    "\n",
    "# Create valid datasets\n",
    "valid_examples = [torchtext.data.Example.fromlist([pair[0], pair[1]], fields) for pair in valid_data]\n",
    "valid_dataset = torchtext.data.Dataset(valid_examples, fields)\n",
    "\n",
    "\n",
    "\n",
    "# # Build vocabulary\n",
    "# engl.build_vocab(dataset, min_freq=2)\n",
    "# TRG.build_vocab(dataset, min_freq=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english:['two', 'young', 'guys', 'with', 'shaggy', 'hair', 'look', 'at', 'their', 'hands', 'while', 'hanging', 'out', 'in', 'the', 'yard', '.'], german:['zwei', 'männer', 'betrachten', 'etwas', 'im', 'garten']\n",
      "english:['the', 'man', 'with', 'pierced', 'ears', 'is', 'wearing', 'glasses', 'and', 'an', 'orange', 'hat', '.'], german:['der', 'mann', 'trägt', 'eine', 'orange', 'wollmütze', '.']\n"
     ]
    }
   ],
   "source": [
    "#checking our datasets \n",
    "eg= train_dataset[0]\n",
    "print(f\"english:{eg.english}, german:{eg.german}\")\n",
    "eg2 = test_dataset[0]\n",
    "print(f\"english:{eg2.english}, german:{eg2.german}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building vocab for our training datas\n",
    "\n",
    "english.build_vocab(train_dataset, max_size= 1000, min_freq=2)\n",
    "german.build_vocab(train_dataset, max_size= 1000, min_freq=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating an encoder class\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_size, num_layers, d):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size= embedding_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(d)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=d)\n",
    "\n",
    "    def forward(self,x):\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        output,(hidden,cell) = self.rnn(embedding)\n",
    "        return hidden,cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a decoder class\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_size, output_size, num_layers,d):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.dropout = nn.Dropout(d)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=d)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
    "        # is 1 here because we are sending in a single word and not a sentence\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # outputs shape: (1, N, hidden_size)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "\n",
    "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
    "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
    "        # just gonna remove the first dim\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden, cell\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now combining our encoder and decoder into seq2seq module\n",
    "\n",
    "class Seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        \n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(english.vocab)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "\n",
    "        #passing source through encoder\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "     #After this grab the first <sos> of the target\n",
    "        x= target[0]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            #Now feed target and the data from hidden and cell  into the decoder\n",
    "            output, hidden, cell = self.decoder(x,hidden, cell)\n",
    "\n",
    "            #storing the predicted values \n",
    "            outputs[t]= output\n",
    "\n",
    "            #now selecting the best guess among the result\n",
    "            best_result = output.argmax(1)\n",
    "\n",
    "            #using the teacher learning ratio for the best and smooth predictions \n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_result\n",
    "\n",
    "        return outputs    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "load_model = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size_encoder = len(german.vocab)\n",
    "input_size_decoder = len(english.vocab)\n",
    "output_size = len(english.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024  # Needs to be the same for both RNN's\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_net = Encoder(input_size_encoder,hidden_size, encoder_embedding_size, num_layers,enc_dropout).to(device)\n",
    "decoder_net = Decoder(input_size_decoder,hidden_size, decoder_embedding_size,output_size, num_layers,enc_dropout).to(device)\n",
    "model = Seq2seq(encoder_net, decoder_net).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (embedding): Embedding(1004, 300)\n",
      "  (rnn): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
      ")\n",
      "\n",
      "Decoder(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (embedding): Embedding(1004, 300)\n",
      "  (rnn): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
      "  (fc): Linear(in_features=1024, out_features=1004, bias=True)\n",
      ")\n",
      "\n",
      "Seq2seq(\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (embedding): Embedding(1004, 300)\n",
      "    (rnn): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (embedding): Embedding(1004, 300)\n",
      "    (rnn): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
      "    (fc): Linear(in_features=1024, out_features=1004, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(encoder_net)\n",
    "print(\"\")\n",
    "print(decoder_net)\n",
    "print(\"\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 2\n",
    "learning_rate = 0.001\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creating our iterator for taining \n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_dataset, valid_dataset, test_dataset),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.english),\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "pad_idx = english.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the tensor board\n",
    "\n",
    "writer= SummaryWriter()\n",
    "step=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0a52e50d484aa7a4e0055766ce91fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 / 2]\n",
      "=> Saving checkpoint\n",
      "cuda\n",
      "Translated example sentence: \n",
      " ['kicking', 'goggles', 'goggles', 'goggles', 'goggles', 'goggles', 'about', 'lined', 'goggles', 'goggles', 'that', 'goggles', 'goggles', 'goggles', 'that', 'goggles', 'goggles', 'goggles', 'that', 'goggles', 'goggles', 'goggles', 'that', 'goggles', 'goggles', 'goggles', 'that', 'goggles', 'goggles', 'goggles', 'that', 'goggles', 'goggles', 'goggles', 'that', 'goggles', 'goggles', 'goggles', 'that', 'goggles', 'goggles', 'goggles', 'that', 'goggles', 'goggles', 'goggles', 'that', 'goggles', 'goggles', 'goggles']\n",
      "[Epoch 1 / 2]\n",
      "=> Saving checkpoint\n",
      "cuda\n",
      "Translated example sentence: \n",
      " ['a', 'man', 'in', 'a', 'black', 'shirt', 'and', 'a', 'a', '<unk>', '<unk>', 'a', '<unk>', '<unk>', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "if load_model:\n",
    "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "\n",
    "\n",
    "sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\"\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint)\n",
    "    print(device)\n",
    "    model.eval()\n",
    "\n",
    "    translated_sentence = translate_sentence(\n",
    "        model, sentence, german, english, device, max_length=50\n",
    "    )\n",
    "\n",
    "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        # Get input and targets and get to cuda\n",
    "        inp_data = batch.german.to(device)\n",
    "        target = batch.english.to(device)\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(inp_data, target)\n",
    "\n",
    "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
    "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "        # way that we have output_words * batch_size that we want to send in into\n",
    "        # our cost function, so we need to do some reshapin. While we're at it\n",
    "        # Let's also remove the start token while we're at it\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Plot to tensorboard\n",
    "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        step += 1\n",
    "\n",
    "\n",
    "\n",
    "score = bleu(test_data[1:100], model, german, english, device)\n",
    "print(f\"Bleu score {score*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Our model\n",
    "torch.load(\"my_checkpoint.pth.tar\")\n",
    "test_sentence = \"sie ist grausam\"\n",
    "model.eval()\n",
    "translated_sentence = translate_sentence(\n",
    "    model, test_sentence, german, english, device, max_length=50\n",
    ")\n",
    "\n",
    "print(f\"Translated example sentence: \\n {translated_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the bleu score\n",
    "targets = []\n",
    "outputs = []\n",
    "for  example in tqdm(test_dataset.examples[1:100]):\n",
    "\n",
    "      src = example.german\n",
    "      trg = example.english\n",
    "      # print(src)\n",
    "      prediction = translate_sentence(model, src, german, english, device)\n",
    "      # print(prediction[:-1])\n",
    "      targets.append([trg])\n",
    "      outputs.append(prediction)\n",
    "print(bleu_score(outputs, targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
